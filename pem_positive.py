import random
import time

import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import os

import warnings
import Tools.Positive

# è®¾ç½®ä¸­æ–‡å­—ä½“
from EnergeModel.Tools import Config, DataReader
from EnergeModel.Tools.RasterProcessor import RasterProcessor

from tqdm import tqdm

plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

def seed_everything(seed=42):
    """å›ºå®šæ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¤šGPUæƒ…å†µ
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # è®¾ç½®PyTorch Lightningçš„éšæœºç§å­
    pl.seed_everything(seed, workers=True)

def reset_seed(seed=42):
    """é‡ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # å¤šGPUæƒ…å†µ
    # è®¾ç½®ç¯å¢ƒå˜é‡å’ŒCuDNN
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # æˆ– ':16:8'
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # å¯¹äºPyTorch 1.7+
    if hasattr(torch, 'use_deterministic_algorithms'):
        torch.use_deterministic_algorithms(True)

# ----------------------------
# æ•°æ®é›†
# ----------------------------
class PositiveDataset(Dataset):
    def __init__(self, X_pos: torch.Tensor, X_bg: torch.Tensor):
        self.X_pos = X_pos.cpu()
        self.X_bg = X_bg.cpu()

    def __len__(self):
        return 1  # å…¨æ‰¹é‡è®­ç»ƒ

    def __getitem__(self, idx):
        return self.X_pos, self.X_bg


class PEMModel(pl.LightningModule):
    def __init__(self, input_dim: int):
        super().__init__()
        self.save_hyperparameters()


        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1, bias=False)
        )



        # åŠ¨æ€å‚æ•°
        self.margin = 2.0
        self.temperature = 2.0

        self.l2w=0.08
        self.adjustment_rate=0.05

        self.training_history = {
            'epoch': [],
            'margin': [],
            'temperature': [],
            'e_pos_mean': [],
            'e_bg_mean': [],
            'e_bg_std': []
        }

        # åˆå§‹åŒ–
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.apply(init_weights)

    def forward(self, x):
        return self.encoder(x)

    def __call__(self, x):
        return self.predict_proba(x)

    def predict_proba(self, x):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆè‡ªåŠ¨å¤„ç†è®¾å¤‡é—®é¢˜ï¼‰"""
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨CPUä¸Š
            if isinstance(x, torch.Tensor):
                if x.is_cuda:
                    x = x.cpu()
                x = x.numpy()
            elif isinstance(x, np.ndarray):
                x = x.astype(np.float32)
            else:

                raise TypeError("è¾“å…¥å¿…é¡»æ˜¯numpyæ•°ç»„æˆ–PyTorchå¼ é‡")
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶æ”¾åˆ°æ¨¡å‹è®¾å¤‡ä¸Š
            x_tensor = torch.FloatTensor(x).to(self.device)
            energy = self.encoder(x_tensor)
            probs = torch.sigmoid(-energy / self.temperature).cpu().numpy()
            return probs.flatten()


    def training_step(self, batch, batch_idx):
        x_pos, x_bg = batch
        x_pos = x_pos.to(self.device)
        x_bg = x_bg.to(self.device)
        e_pos = self.forward(x_pos)
        e_bg = self.forward(x_bg)

        scaled_e_pos = e_pos / self.temperature
        scaled_e_bg = e_bg / self.temperature

        prob_pos = torch.sigmoid(-scaled_e_pos)
        prob_bg = torch.sigmoid(-scaled_e_bg)

        pos_mean = scaled_e_pos.mean()
        bg_mean = scaled_e_bg.mean()

        contrast_loss = F.softplus((pos_mean - bg_mean + self.margin) * 0.5)
        energy_reg = self.l2w * (scaled_e_pos.pow(2).mean() + scaled_e_bg.pow(2).mean())
        loss = contrast_loss  + energy_reg

        # åŠ¨æ€è°ƒæ•´å‚æ•°

        if self.current_epoch % 10 == 0:
            self.training_history['epoch'].append(self.current_epoch)
            self.training_history['margin'].append(self.margin)
            self.training_history['temperature'].append(self.temperature)
            self.training_history['e_pos_mean'].append(e_pos.mean().item())
            self.training_history['e_bg_mean'].append(e_bg.mean().item())
            self.training_history['e_bg_std'].append(e_bg.std().item())

        self.log_dict({
            'train_loss': loss,
            'pos_prob': prob_pos.mean(),
            'bg_prob': prob_bg.mean(),
            'bg_prob_std': prob_bg.std(),
            'margin': self.margin,
            'temperature': self.temperature,
        }, prog_bar=True)

        return loss

    def on_train_epoch_end(self):
        """åœ¨è®­ç»ƒepochç»“æŸåè¿›è¡Œå‚æ•°è°ƒæ•´ï¼Œé¿å…ä¸æ¢¯åº¦è®¡ç®—å†²çª"""
        if self.current_epoch % 20 == 0:
            # è·å–å½“å‰batchçš„ç»Ÿè®¡é‡ï¼ˆéœ€è¦è®°å½•æˆ–é‡æ–°è®¡ç®—ï¼‰
            # è¿™é‡Œéœ€è¦ä¿®æ”¹ï¼šæˆ‘ä»¬éœ€è¦åœ¨training_stepä¸­è®°å½•å¿…è¦çš„ç»Ÿè®¡é‡
            pass

    def configure_optimizers(self):
        opt = torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.005)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            opt, mode='min', factor=0.5, patience=10, verbose=True)
        return {
            "optimizer": opt,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "train_loss",
                "interval": "epoch",
                "frequency": 1
            },
            "gradient_clip_val": 0.5
        }

    def save_training_history(self, output_dir: str):
        """ä¿å­˜è®­ç»ƒå†å²æ•°æ®åˆ°Excelæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        history_file = os.path.join(output_dir, "training_history.xlsx")

        # åˆ›å»ºDataFrame
        df_history = pd.DataFrame(self.training_history)

        # ä¿å­˜åˆ°Excel
        df_history.to_excel(history_file, index=False)
        print(f"âœ… è®­ç»ƒå†å²æ•°æ®å·²ä¿å­˜è‡³: {os.path.abspath(history_file)}")

        return df_history

    def get_feature_importance(self):
        return None

# ----------------------------
# æ¨¡å‹
# ----------------------------
class AutoPEMModel(pl.LightningModule):
    def __init__(self, input_dim: int):
        super().__init__()
        self.save_hyperparameters()

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1, bias=False)
        )
        # åŠ¨æ€å‚æ•°
        self.margin = nn.Parameter(torch.tensor(2.0))
        self.temperature = nn.Parameter(torch.tensor(2.0))

        self.l2w=0.08
        self.adjustment_rate=0.05

        self.training_history = {
            'epoch': [],
            'margin': [],
            'temperature': [],
            'e_pos_mean': [],
            'e_bg_mean': [],
            'e_bg_std': []
        }

        # åˆå§‹åŒ–
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.apply(init_weights)


    def forward(self, x):
        return self.encoder(x)

    def __call__(self, x):
        return self.predict_proba(x)

    def predict_proba(self, x):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆè‡ªåŠ¨å¤„ç†è®¾å¤‡é—®é¢˜ï¼‰"""
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨CPUä¸Š
            if isinstance(x, torch.Tensor):
                if x.is_cuda:
                    x = x.cpu()
                x = x.numpy()
            elif isinstance(x, np.ndarray):
                x = x.astype(np.float32)
            else:
                raise TypeError("è¾“å…¥å¿…é¡»æ˜¯numpyæ•°ç»„æˆ–PyTorchå¼ é‡")

            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶æ”¾åˆ°æ¨¡å‹è®¾å¤‡ä¸Š
            x_tensor = torch.FloatTensor(x).to(self.device)
            energy = self.encoder(x_tensor)
            probs = torch.sigmoid(-energy / self.temperature).cpu().numpy()
            return probs.flatten()

    def training_step(self, batch, batch_idx):
        x_pos, x_bg = batch
        x_pos = x_pos.to(self.device)
        x_bg = x_bg.to(self.device)
        e_pos = self.forward(x_pos)
        e_bg = self.forward(x_bg)

        scaled_e_pos = e_pos / self.temperature
        scaled_e_bg = e_bg / self.temperature

        prob_pos = torch.sigmoid(-scaled_e_pos)
        prob_bg = torch.sigmoid(-scaled_e_bg)

        pos_mean = scaled_e_pos.mean()
        bg_mean = scaled_e_bg.mean()

        contrast_loss = F.softplus((pos_mean - bg_mean + self.margin) * 0.5)
        energy_reg = self.l2w * (scaled_e_pos.pow(2).mean() + scaled_e_bg.pow(2).mean())
        loss = contrast_loss  + energy_reg

        if self.current_epoch % 20 == 0:
            self.training_history['epoch'].append(self.current_epoch)
            self.training_history['margin'].append(self.margin.item())
            self.training_history['temperature'].append(self.temperature.item())
            self.training_history['e_pos_mean'].append(e_pos.mean().item())
            self.training_history['e_bg_mean'].append(e_bg.mean().item())
            self.training_history['e_bg_std'].append(e_bg.std().item())

        self.log_dict({
            'train_loss': loss,
            'pos_prob': prob_pos.mean(),
            'bg_prob': prob_bg.mean(),
            'bg_prob_std': prob_bg.std(),
            'margin': self.margin,
            'temperature': self.temperature,
        }, prog_bar=True)

        return loss

    def on_train_epoch_end(self):
        """åœ¨è®­ç»ƒepochç»“æŸåè¿›è¡Œå‚æ•°è°ƒæ•´ï¼Œé¿å…ä¸æ¢¯åº¦è®¡ç®—å†²çª"""
        if self.current_epoch % 20 == 0:
            # è·å–å½“å‰batchçš„ç»Ÿè®¡é‡ï¼ˆéœ€è¦è®°å½•æˆ–é‡æ–°è®¡ç®—ï¼‰
            # è¿™é‡Œéœ€è¦ä¿®æ”¹ï¼šæˆ‘ä»¬éœ€è¦åœ¨training_stepä¸­è®°å½•å¿…è¦çš„ç»Ÿè®¡é‡
            pass

    def configure_optimizers(self):
        opt = torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.005)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            opt, mode='min', factor=0.5, patience=8, verbose=True)
        return {
            "optimizer": opt,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "train_loss",
                "interval": "epoch",
                "frequency": 1
            },
            "gradient_clip_val": 0.5
        }

    def save_training_history(self, output_dir: str):
        """ä¿å­˜è®­ç»ƒå†å²æ•°æ®åˆ°Excelæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        history_file = os.path.join(output_dir, "training_history.xlsx")

        # åˆ›å»ºDataFrame
        df_history = pd.DataFrame(self.training_history)

        # ä¿å­˜åˆ°Excel
        df_history.to_excel(history_file, index=False)
        print(f"âœ… è®­ç»ƒå†å²æ•°æ®å·²ä¿å­˜è‡³: {os.path.abspath(history_file)}")

        return df_history

    def get_feature_importance(self):
        return None

class PEM_Shallow(PEMModel):
    """
    æ›´æµ…çš„æ¶æ„å˜ä½“ (256-128)
    ç»§æ‰¿è‡ª PEMModelï¼Œä½†ä½¿ç”¨æ›´å°‘çš„éšè—å±‚
    """

    def __init__(self, input_dim: int):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†éšåä¼šè¦†ç›–encoder
        super().__init__(input_dim)

        # é‡å†™ç¼–ç å™¨ä¸ºæ›´æµ…çš„æ¶æ„
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),  # ç¬¬ä¸€å±‚ï¼š256ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),  # ç¬¬äºŒå±‚ï¼š128ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1, bias=False)  # è¾“å‡ºå±‚
        )

        # é‡æ–°åˆå§‹åŒ–æ–°ç¼–ç å™¨çš„æƒé‡
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.encoder.apply(init_weights)

class PEM_Deep(PEMModel):
    """
    æ›´æ·±çš„æ¶æ„å˜ä½“ (1024-512-256-128)
    ç»§æ‰¿è‡ª PEMModelï¼Œä½†å¢åŠ ç½‘ç»œæ·±åº¦
    """

    def __init__(self, input_dim: int):
        super().__init__(input_dim)

        # é‡å†™ç¼–ç å™¨ä¸ºæ›´æ·±çš„æ¶æ„
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),  # ç¬¬ä¸€å±‚ï¼š1024ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),  # ç¬¬äºŒå±‚ï¼š512ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),  # ç¬¬ä¸‰å±‚ï¼š256ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),  # ç¬¬å››å±‚ï¼š128ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1, bias=False)  # è¾“å‡ºå±‚
        )

        # é‡æ–°åˆå§‹åŒ–æƒé‡
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.encoder.apply(init_weights)

class PEM_Wide(PEMModel):
    """
    æ›´å®½ä½†æ›´æµ…çš„æ¶æ„å˜ä½“ (1024-1024)
    ç»§æ‰¿è‡ª PEMModelï¼Œä½¿ç”¨æ›´å®½çš„å±‚ä½†å‡å°‘æ·±åº¦
    """

    def __init__(self, input_dim: int):
        super().__init__(input_dim)

        # é‡å†™ç¼–ç å™¨ä¸ºæ›´å®½ä½†æ›´æµ…çš„æ¶æ„
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),  # ç¬¬ä¸€å±‚ï¼š1024ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 512),  # ç¬¬äºŒå±‚ï¼š1024ä¸ªç¥ç»å…ƒï¼ˆä¿æŒå®½åº¦ï¼‰
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 1, bias=False)  # è¾“å‡ºå±‚
        )

        # é‡æ–°åˆå§‹åŒ–æƒé‡
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.encoder.apply(init_weights)

class PEM_Narrow(PEMModel):
    """
    æ›´çª„çš„ç“¶é¢ˆç»“æ„å˜ä½“ (256-128-64)
    ç»§æ‰¿è‡ª PEMModelï¼Œä½¿ç”¨æ›´çª„çš„å±‚æ„é€ ç“¶é¢ˆç»“æ„
    """

    def __init__(self, input_dim: int):
        super().__init__(input_dim)

        # é‡å†™ç¼–ç å™¨ä¸ºæ›´çª„çš„ç“¶é¢ˆæ¶æ„
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),  # ç¬¬ä¸€å±‚ï¼š256ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),  # ç¬¬äºŒå±‚ï¼š128ä¸ªç¥ç»å…ƒ
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),  # ç¬¬ä¸‰å±‚ï¼š64ä¸ªç¥ç»å…ƒï¼ˆç“¶é¢ˆå±‚ï¼‰
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, 1, bias=False)  # è¾“å‡ºå±‚
        )

        # é‡æ–°åˆå§‹åŒ–æƒé‡
        def init_weights(m):
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

        self.encoder.apply(init_weights)

def train_and_evaluate(
        model,
        pos_path: str,
        bg_path: str,
        test_path: str = None,  # æ–°å¢æµ‹è¯•é›†è·¯å¾„
        output_dir: str = "results",
        max_epochs=1000
) -> dict:
    """è®­ç»ƒè¯„ä¼°æµç¨‹ï¼ˆä½¿ç”¨èƒŒæ™¯æ ·æœ¬åˆ’åˆ†é£é™©åŒºåŸŸï¼‰"""
    os.makedirs(output_dir, exist_ok=True)
    # æ•°æ®åŠ è½½
    df_pos, df_bg, df_test, feature_names = Tools.Positive.load_data(pos_path, bg_path, test_path)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ç”¨äºè®­ç»ƒ
    X_pos = df_pos.values.astype(np.float32)
    X_bg = df_bg.values.astype(np.float32)
    X_pos = torch.FloatTensor(X_pos)
    X_bg = torch.FloatTensor(X_bg)
    X_test = df_test.values.astype(np.float32)

    print(f"æ­£æ ·æœ¬æ•°é‡: {len(X_pos)}")
    print(f"èƒŒæ™¯æ ·æœ¬æ•°é‡: {len(df_bg)}")
    if df_test is not None:
        print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(df_test)}")
    print(f"ç‰¹å¾ç»´åº¦: {X_pos.shape[1]}")
    print(f"ç‰¹å¾å: {feature_names}")

    # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")

    dataset = PositiveDataset(X_pos, X_bg)
    train_loader = DataLoader(dataset, batch_size=None, shuffle=False)

    # å›è°ƒé…ç½®
    checkpoint = ModelCheckpoint(
        dirpath=output_dir,
        filename="best_model",
        monitor="train_loss",
        mode="min",
        save_top_k=1
    )

    early_stop = EarlyStopping(
        monitor="train_loss",
        patience=30,
        mode="min",
        verbose=True
    )
    # è®­ç»ƒå™¨é…ç½®
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator="auto",
        devices="auto",
        callbacks=[checkpoint],
        default_root_dir=output_dir,
        deterministic=True,
        precision="16-mixed",
        accumulate_grad_batches=4,
        logger=True,
        enable_progress_bar=True
    )
    # è®­ç»ƒ
    trainer.fit(model, train_loader)

    df_history = model.save_training_history(output_dir)

    # è¯„ä¼°
    best_model = model.load_from_checkpoint(
        checkpoint.best_model_path,
        input_dim=X_pos.shape[1]
    )
    # è¯„ä¼°æ¨¡å‹ï¼ˆå¢åŠ æµ‹è¯•é›†è¯„ä¼°ï¼‰
    best_model.eval()
    with torch.no_grad():
        results = Tools.Positive.evaluate_model(best_model, X_pos, X_bg,X_test, df_pos, df_bg, df_test, feature_names, output_dir)

    results['training_history'] = df_history

    return best_model,results


def run_one():
    start_time = time.time()
    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    bg_path = Config.BASE_DIR + "mesh1800.xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"
    output_dir = "result/pem_positive"

    # è¿è¡Œè®­ç»ƒè¯„ä¼°æµç¨‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ­£æ ·æœ¬æ¨¡å‹...")

    df_pos = DataReader.load_data(pos_path)
    model = PEMModel(input_dim=df_pos.shape[1])
    #model = AutoPEMModel(input_dim=df_pos.shape[1])
    model, results = train_and_evaluate(
        model,
        pos_path=pos_path,
        bg_path=bg_path,
        test_path=test_path,  # ä¼ å…¥æµ‹è¯•é›†è·¯å¾„
        output_dir=output_dir,
        max_epochs=500
    )
    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\nâ­ æœ€ç»ˆè¯„ä¼°ç»“æœ â­")
    print(f"â— AD_AUCåˆ†æ•°: {results['ad_auc_score']:.4f}")
    print(f"â— é¢„æµ‹ç²¾åº¦: {results['train_accuracy']:.4f}")
    print(f"â— é¢„æµ‹å¯†åº¦: {results['train_density']:.4f}")
    print(
        f"â— æ­£æ ·æœ¬æ¦‚ç‡ç»Ÿè®¡ - å‡å€¼: {results['pos_prob_mean']:.3f} Â± {results['pos_prob_std']:.3f} | ä¸­ä½æ•°: {results['pos_median']:.3f}")
    print(
        f"â— èƒŒæ™¯æ ·æœ¬æ¦‚ç‡ç»Ÿè®¡ - å‡å€¼: {results['bg_prob_mean']:.3f} Â± {results['bg_prob_std']:.3f} | ä¸­ä½æ•°: {results['bg_median']:.3f}")

    # æ‰“å°æµ‹è¯•é›†ç»“æœ
    if 'test_accuracy' in results:
        print(f"\nğŸ§ª æµ‹è¯•é›†è¯„ä¼°ç»“æœï¼ˆçº¯ç¾å®³æ ·æœ¬ï¼‰:")
        print(f"â— AD_AUCåˆ†æ•°: {results['ad_auc_score_test']:.4f}")
        print(f"â— åˆ†ç±»å‡†ç¡®ç‡: {results['test_accuracy']:.2%}")  # ç™¾åˆ†æ¯”æ ¼å¼æ›´ç›´è§‚
        print(f"â— æµ‹è¯•æ ·æœ¬æ•°: {results['test_size']}")

    print(f"\nğŸ“Š é£é™©åŒºåŸŸåˆ†æç»“æœ:")
    print("=" * 80)
    print(f"{'é£é™©åŒºåŸŸ':<12} {'é˜ˆå€¼èŒƒå›´':<20} {'ç¾å®³ç‚¹æ•°é‡':<10} {'ç¾å®³ç‚¹æ¯”ä¾‹':<12} {'èƒŒæ™¯æ ·æœ¬æ¯”ä¾‹':<12}")
    print("-" * 80)

    for risk_name, risk_info in results['risk_zones'].items():
        print(f"{risk_name:<12} {risk_info['é˜ˆå€¼èŒƒå›´']:<20} {risk_info['ç¾å®³ç‚¹æ•°é‡']:<10} "
              f"{risk_info['ç¾å®³ç‚¹æ¯”ä¾‹']:<12.1%} {risk_info['èƒŒæ™¯æ ·æœ¬æ¯”ä¾‹']:<12.1%}")

    print(f"â— ç»“æœä¿å­˜è·¯å¾„: {os.path.abspath(output_dir)}")

    if Config.EXPORT_TIFF:
        df_pos = DataReader.load_data(pos_path)
        feature_names = df_pos.columns.tolist()  # å‡è®¾df_poså·²ä»pos_pathåŠ è½½
        # æ„å»ºç‰¹å¾æ˜ å°„å­—å…¸ï¼ˆè‡ªåŠ¨åŒ¹é…entropiesç›®å½•ä¸‹çš„åŒåtifï¼‰
        feature_mapping = {
            feature: os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif")
            for feature in feature_names
            if os.path.exists(os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif"))
        }
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = RasterProcessor(model, feature_mapping)

        # è¾“å‡ºè·¯å¾„
        prob_tif_path = os.path.join(output_dir, "susceptibility_probability.tif")

        # æ‰§è¡Œé¢„æµ‹
        #processor.predict_to_raster(prob_tif_path)
        processor.predict_to_raster_with_filter(5,prob_tif_path)
        print(f"âœ… ç©ºé—´æ¦‚ç‡åˆ†å¸ƒå·²ä¿å­˜è‡³: {os.path.abspath(prob_tif_path)}")
        risk_thresholds = {
            zone_name: {
                'ä¸‹é™é˜ˆå€¼': float(zone_info['é˜ˆå€¼èŒƒå›´'].split(' - ')[0]),
                'ä¸Šé™é˜ˆå€¼': float(zone_info['é˜ˆå€¼èŒƒå›´'].split(' - ')[1])
            }
            for zone_name, zone_info in results['risk_zones'].items()
        }

        zone_colors = ['red', 'orange', 'yellow', 'lightgreen', 'lightblue']
        zones_tif_path = os.path.join(output_dir, "susceptibility_zones.tif")
        zones = processor.generate_susceptibility_zones(
            prob_tif_path=prob_tif_path,
            risk_thresholds=risk_thresholds,
            output_tif_path=zones_tif_path,
            colors=zone_colors
        )

    end_time = time.time()
    total_time = end_time - start_time

    # æ ¼å¼åŒ–æ˜¾ç¤ºè¿è¡Œæ—¶é—´
    time_str = f"{total_time:.2f}ç§’"
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼æ€»è¿è¡Œæ—¶é—´: {time_str}")
    print("=" * 60)

def batch_training_experiment():
    """æ‰¹é‡è®­ç»ƒå®éªŒï¼šå¯¹ä¸åŒç½‘æ ¼æ–‡ä»¶è¿›è¡Œè®­ç»ƒå¹¶è®°å½•æŒ‡æ ‡ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œå•æ¬¡è®­ç»ƒï¼‰"""
    start_time = time.time()

    # é…ç½®ä¸åŒçš„ç½‘æ ¼æ–‡ä»¶è·¯å¾„
    mesh_files = [
        "mesh600.xlsx", "mesh1200.xlsx", "mesh1800.xlsx", "mesh2400.xlsx",
        "mesh3000.xlsx", "mesh3600.xlsx", "mesh4200.xlsx", "mesh4800.xlsx",
        "mesh5400.xlsx", "mesh6000.xlsx"
    ]
    # å›ºå®šæ–‡ä»¶è·¯å¾„
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"

    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = []

    # å¯¹æ¯ä¸ªç½‘æ ¼æ–‡ä»¶è¿›è¡Œ1æ¬¡è®­ç»ƒï¼ˆç¡®å®šæ€§è®¡ç®—ï¼‰
    for mesh_file in mesh_files:
        reset_seed()

        bg_path = Config.BASE_DIR + mesh_file
        mesh_name = mesh_file.replace('.xlsx', '')

        print(f"\n{'=' * 60}")
        print(f"ğŸ”¬ å¼€å§‹å¤„ç†ç½‘æ ¼æ–‡ä»¶: {mesh_file}")
        print(f"{'=' * 60}")

        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = f"result/pem_positive/pem_positive_{mesh_name}"
            os.makedirs(output_dir, exist_ok=True)

            # åŠ è½½æ­£æ ·æœ¬æ•°æ®è·å–ç‰¹å¾ç»´åº¦
            df_pos = DataReader.load_data(pos_path)
            model = PEMModel(input_dim=df_pos.shape[1])

            # ä½¿ç”¨ç®€åŒ–è®­ç»ƒå‡½æ•°
            best_model, metrics = train_and_evaluate(
                model=model,
                pos_path=pos_path,
                bg_path=bg_path,
                test_path=test_path,  # ä¸ä½¿ç”¨æµ‹è¯•é›†ä»¥åŠ å¿«è®­ç»ƒ
                output_dir=output_dir,
                max_epochs=1000  # é€‚å½“å‡å°‘epochsä»¥æé«˜æ•ˆç‡
            )

            # ä¿å­˜å…³é”®æŒ‡æ ‡
            result = {
                'mesh_file': mesh_file,
                'ad_auc_score': metrics['ad_auc_score'],
                'ad_auc_score_test': metrics['ad_auc_score_test'],
                'train_accuracy': metrics['train_accuracy'],
                'train_density': metrics['train_density']
            }

            all_results.append(result)

            print(f"âœ… è®­ç»ƒå®Œæˆ - AD_AUC: {metrics['ad_auc_score']:.4f}, "
                  f"å‡†ç¡®ç‡: {metrics['train_accuracy']:.4f}, å¯†åº¦: {metrics['train_density']:.4f}")

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            # è®°å½•å¤±è´¥ä¿¡æ¯
            result = {
                'mesh_file': mesh_file,
                'ad_auc_score': 0.0,
                'ad_auc_score_test': 0.0,
                'train_accuracy': 0.0,
                'train_density': 0.0,
                'mean_pos_prob': 0.0,
                'error': str(e)
            }
            all_results.append(result)

    # ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶
    results_df = pd.DataFrame(all_results)

    # ä¿å­˜åˆ°Excel
    output_excel_path = "result/pem_positive/batch_training_results.xlsx"
    os.makedirs(os.path.dirname(output_excel_path), exist_ok=True)
    results_df.to_excel(output_excel_path, index=False)

    end_time = time.time()
    total_time = end_time - start_time

    # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
    print(f"\n{'=' * 80}")
    print(f"ğŸ‰ æ‰¹é‡è®­ç»ƒå®éªŒå®Œæˆ!")
    print(f"{'=' * 80}")
    print(f"ğŸ“Š å®éªŒæ¦‚å†µ:")
    print(f"â— å¤„ç†çš„ç½‘æ ¼æ–‡ä»¶æ•°é‡: {len(mesh_files)}")
    print(f"â— æ€»è¿è¡Œæ—¶é—´: {total_time / 60:.2f} åˆ†é’Ÿ")
    print(f"â— ç»“æœæ–‡ä»¶: {output_excel_path}")

    # ç»Ÿè®¡æˆåŠŸè®­ç»ƒçš„æ•°é‡
    successful_runs = len([r for r in all_results if r.get('ad_auc_score', 0) > 0])
    print(f"â— æˆåŠŸè®­ç»ƒæ¬¡æ•°: {successful_runs}/{len(mesh_files)}")

    print(f"\nğŸ† æ€§èƒ½æœ€ä½³çš„å‰3ä¸ªç½‘æ ¼æ–‡ä»¶:")
    top_3 = results_df.head(3)
    for i, (_, row) in enumerate(top_3.iterrows()):
        print(f"{i + 1}. {row['mesh_file']}: AD_AUC = {row['ad_auc_score']:.4f}")

    return results_df

def run_m_T():
    start_time = time.time()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    bg_path = Config.BASE_DIR + "mesh3000.xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"
    output_base_dir = "result/pem_positive"

    # å®šä¹‰ä¸åŒçš„ m å’Œ T ç»„åˆ
    param_combinations = [
        (0.25, 16.0),  # ä½œä¸ºå¯¹æ¯”
        (1.0, 8.0),  # ä½œä¸ºå¯¹æ¯”
        (1.0, 4.0),  # ä½œä¸ºå¯¹æ¯”
        (2.0, 4.0),  # ä½œä¸ºå¯¹æ¯”
        (2.0, 2.0),  # ä½œä¸ºå¯¹æ¯”
        (4.0, 2.0),  # ä½œä¸ºå¯¹æ¯”
        (8.0, 1.0),  # ä½œä¸ºå¯¹æ¯”
        (16.0, 0.5),  # ä½œä¸ºå¯¹æ¯”
    ]
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []

    # åŠ è½½æ•°æ®ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
    df_pos, df_bg, df_test, feature_names = Tools.Positive.load_data(pos_path, bg_path, test_path)
    X_pos = df_pos.values.astype(np.float32)
    X_bg = df_bg.values.astype(np.float32)

    print(f"æ­£æ ·æœ¬æ•°é‡: {len(X_pos)}")
    print(f"èƒŒæ™¯æ ·æœ¬æ•°é‡: {len(df_bg)}")
    if df_test is not None:
        print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(df_test)}")
    print(f"ç‰¹å¾ç»´åº¦: {X_pos.shape[1]}")
    print(f"å‚æ•°ç»„åˆæ•°é‡: {len(param_combinations)}æ¬¡å®éªŒ")
    print("=" * 80)

    # éå†æ‰€æœ‰å‚æ•°ç»„åˆï¼ˆæ¯ä¸ªç»„åˆåªè®­ç»ƒ1æ¬¡ï¼‰
    for i, (initial_margin, initial_temperature) in enumerate(param_combinations):
        reset_seed()
        print(f"\nğŸ”¬ æ­£åœ¨è®­ç»ƒå‚æ•°ç»„åˆ {i + 1}/{len(param_combinations)}: m={initial_margin}, T={initial_temperature}")

        # ä¸ºå½“å‰å‚æ•°ç»„åˆåˆ›å»ºä¸“é—¨çš„è¾“å‡ºç›®å½•
        param_output_dir = os.path.join(output_base_dir, f"m_{initial_margin}_T_{initial_temperature}")
        os.makedirs(param_output_dir, exist_ok=True)

        try:
            # åˆ›å»ºæ¨¡å‹å¹¶è®¾ç½®åˆå§‹å‚æ•°
            model = PEMModel(input_dim=X_pos.shape[1])

            # æ‰‹åŠ¨è®¾ç½®åˆå§‹å‚æ•°ï¼ˆè¦†ç›–åˆå§‹åŒ–å€¼ï¼‰
            with torch.no_grad():
                model.margin = initial_margin
                model.temperature = initial_temperature

            # è®­ç»ƒæ¨¡å‹
            trained_model, results = train_and_evaluate(
                model,
                pos_path=pos_path,
                bg_path=bg_path,
                test_path=test_path,
                output_dir=param_output_dir
            )

            # æ”¶é›†ç»“æœ
            run_result = {
                'å‚æ•°ç»„åˆ': f'm={initial_margin}, T={initial_temperature}',
                'AD_AUCåˆ†æ•°': results.get('ad_auc_score', 0),
                'æµ‹è¯•AD_AUCåˆ†æ•°': results.get('ad_auc_score_test', 0),
                'é¢„æµ‹ç²¾åº¦': results.get('train_accuracy', 0),
                'é¢„æµ‹å¯†åº¦': results.get('train_density', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡å‡å€¼': results.get('pos_prob_mean', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': results.get('pos_prob_std', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': results.get('pos_median', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡å‡å€¼': results.get('bg_prob_mean', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': results.get('bg_prob_std', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': results.get('bg_median', 0),
                'æœ€ç»ˆmargin': model.margin if hasattr(model, 'margin') else initial_margin,
                'æœ€ç»ˆtemperature': model.temperature if hasattr(model, 'temperature') else initial_temperature,
                'è¾“å‡ºè·¯å¾„': param_output_dir
            }

            # æ·»åŠ æµ‹è¯•é›†ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'test_accuracy' in results:
                run_result['æµ‹è¯•é›†å‡†ç¡®ç‡'] = results['test_accuracy']
                run_result['æµ‹è¯•æ ·æœ¬æ•°'] = results['test_size']

            all_results.append(run_result)

            print(f"âœ… è®­ç»ƒå®Œæˆ - TEST-AD-AUC: {run_result['æµ‹è¯•AD_AUCåˆ†æ•°']:.4f}, "
                  f"ç²¾åº¦: {run_result['é¢„æµ‹ç²¾åº¦']:.4f}, å¯†åº¦: {run_result['é¢„æµ‹å¯†åº¦']:.4f}")

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            # è®°å½•å¤±è´¥ä¿¡æ¯
            failed_result = {
                'å‚æ•°ç»„åˆ': f'm={initial_margin}, T={initial_temperature}',
                'AD_AUCåˆ†æ•°': 0,
                'æµ‹è¯•AD_AUCåˆ†æ•°': 0,
                'é¢„æµ‹ç²¾åº¦': 0,
                'é¢„æµ‹å¯†åº¦': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡å‡å€¼': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡å‡å€¼': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': 0,
                'æœ€ç»ˆmargin': initial_margin,
                'æœ€ç»ˆtemperature': initial_temperature,
                'è¾“å‡ºè·¯å¾„': param_output_dir,
                'çŠ¶æ€': f'å¤±è´¥: {e}'
            }
            all_results.append(failed_result)

    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°Excel
    if all_results:
        results_df = pd.DataFrame(all_results)
        results_file = os.path.join(output_base_dir, "all_parameter_results.xlsx")
        results_df.to_excel(results_file, index=False)

        print(f"\nâœ… æ‰€æœ‰å®éªŒç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(results_file)}")

        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ å‚æ•°è°ƒä¼˜å®éªŒç»“æœæ±‡æ€»:")
        print("=" * 100)

        # åªæ˜¾ç¤ºæˆåŠŸçš„å®éªŒ
        successful_results = [r for r in all_results if r.get('æµ‹è¯•AD_AUCåˆ†æ•°', 0) > 0]

        if successful_results:
            # æŒ‰AD-AUCåˆ†æ•°é™åºæ’åˆ—
            successful_results.sort(key=lambda x: x['æµ‹è¯•AD_AUCåˆ†æ•°'], reverse=True)

            print(
                f"{'æ’å':<4} {'å‚æ•°ç»„åˆ':<20} {'TEST-AD-AUC':<8} {'é¢„æµ‹ç²¾åº¦':<8} {'é¢„æµ‹å¯†åº¦':<8} {'æœ€ç»ˆmargin':<10} {'æœ€ç»ˆtemperature':<12}")
            print("-" * 100)

            for i, result in enumerate(successful_results, 1):
                print(f"{i:<4} {result['å‚æ•°ç»„åˆ']:<20} {result['æµ‹è¯•AD_AUCåˆ†æ•°']:.4f}   {result['é¢„æµ‹ç²¾åº¦']:.4f}    "
                      f"{result['é¢„æµ‹å¯†åº¦']:.4f}    {result['æœ€ç»ˆmargin']:.4f}      {result['æœ€ç»ˆtemperature']:.4f}")

            # æ‰¾å‡ºæœ€ä½³å‚æ•°ç»„åˆ
            best_result = successful_results[0]
            print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ: {best_result['å‚æ•°ç»„åˆ']}")
            print(f"   æµ‹è¯•AD_AUCåˆ†æ•°: {best_result['æµ‹è¯•AD_AUCåˆ†æ•°']:.4f}")
            print(f"   é¢„æµ‹ç²¾åº¦: {best_result['é¢„æµ‹ç²¾åº¦']:.4f}")
            print(f"   é¢„æµ‹å¯†åº¦: {best_result['é¢„æµ‹å¯†åº¦']:.4f}")
            print(f"   æœ€ç»ˆmarginå€¼: {best_result['æœ€ç»ˆmargin']:.4f}")
            print(f"   æœ€ç»ˆtemperatureå€¼: {best_result['æœ€ç»ˆtemperature']:.4f}")

    # ç©ºé—´é¢„æµ‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰
    if Config.EXPORT_TIFF and all_results:
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        successful_results = [r for r in all_results if r.get('AD_AUCåˆ†æ•°', 0) > 0]
        if successful_results:
            best_result = max(successful_results, key=lambda x: x['AD_AUCåˆ†æ•°'])
            best_model_path = os.path.join(best_result['è¾“å‡ºè·¯å¾„'], "best_model.ckpt")

            if os.path.exists(best_model_path):
                print(f"\nğŸ—ºï¸  ä½¿ç”¨æœ€ä½³å‚æ•°ç»„åˆ {best_result['å‚æ•°ç»„åˆ']} è¿›è¡Œç©ºé—´é¢„æµ‹...")

                # åŠ è½½æœ€ä½³æ¨¡å‹
                best_model = PEMModel.load_from_checkpoint(
                    best_model_path,
                    input_dim=X_pos.shape[1]
                )

                # æ„å»ºç‰¹å¾æ˜ å°„
                df_pos = DataReader.load_data(pos_path)
                feature_names = df_pos.columns.tolist()
                feature_mapping = {
                    feature: os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif")
                    for feature in feature_names
                    if os.path.exists(os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif"))
                }

                # åˆå§‹åŒ–å¤„ç†å™¨å¹¶æ‰§è¡Œé¢„æµ‹
                processor = RasterProcessor(best_model, feature_mapping)
                prob_tif_path = os.path.join(output_base_dir, "best_susceptibility_probability.tif")
                processor.predict_to_raster(prob_tif_path)
                print(f"âœ… æœ€ä½³æ¨¡å‹ç©ºé—´æ¦‚ç‡åˆ†å¸ƒå·²ä¿å­˜è‡³: {os.path.abspath(prob_tif_path)}")

    end_time = time.time()
    total_time = end_time - start_time

    # æ ¼å¼åŒ–æ˜¾ç¤ºè¿è¡Œæ—¶é—´
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = total_time % 60

    if hours > 0:
        time_str = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.1f}ç§’"
    elif minutes > 0:
        time_str = f"{minutes}åˆ†é’Ÿ{seconds:.1f}ç§’"
    else:
        time_str = f"{seconds:.1f}ç§’"

    print(f"\nğŸ‰ å‚æ•°è°ƒä¼˜å®éªŒå®Œæˆï¼æ€»è¿è¡Œæ—¶é—´: {time_str}")
    print("=" * 80)

    return all_results

def run_Auto_m_T():
    start_time = time.time()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    bg_path = Config.BASE_DIR + "mesh3000.xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"
    output_base_dir = "result/pem_positive"

    # å®šä¹‰ä¸åŒçš„ m å’Œ T ç»„åˆ
    param_combinations = [
        (2.0, 4.0),
    ]

    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []

    # åŠ è½½æ•°æ®ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
    df_pos, df_bg, df_test, feature_names = Tools.Positive.load_data(pos_path, bg_path, test_path)
    X_pos = df_pos.values.astype(np.float32)
    X_bg = df_bg.values.astype(np.float32)

    print(f"æ­£æ ·æœ¬æ•°é‡: {len(X_pos)}")
    print(f"èƒŒæ™¯æ ·æœ¬æ•°é‡: {len(df_bg)}")
    if df_test is not None:
        print(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {len(df_test)}")
    print(f"ç‰¹å¾ç»´åº¦: {X_pos.shape[1]}")
    print(f"å‚æ•°ç»„åˆæ•°é‡: {len(param_combinations)}æ¬¡å®éªŒ")
    print("=" * 80)

    # éå†æ‰€æœ‰å‚æ•°ç»„åˆï¼ˆæ¯ä¸ªç»„åˆåªè®­ç»ƒ1æ¬¡ï¼‰
    for i, (initial_margin, initial_temperature) in enumerate(param_combinations):
        reset_seed()
        print(f"\nğŸ”¬ æ­£åœ¨è®­ç»ƒå‚æ•°ç»„åˆ {i + 1}/{len(param_combinations)}: m={initial_margin}, T={initial_temperature}")

        # ä¸ºå½“å‰å‚æ•°ç»„åˆåˆ›å»ºä¸“é—¨çš„è¾“å‡ºç›®å½•
        param_output_dir = os.path.join(output_base_dir, f"m_{initial_margin}_T_{initial_temperature}")
        os.makedirs(param_output_dir, exist_ok=True)

        try:
            # åˆ›å»ºæ¨¡å‹å¹¶è®¾ç½®åˆå§‹å‚æ•°
            model = AutoPEMModel(input_dim=X_pos.shape[1])

            # æ‰‹åŠ¨è®¾ç½®åˆå§‹å‚æ•°ï¼ˆè¦†ç›–åˆå§‹åŒ–å€¼ï¼‰
            model.margin = initial_margin
            model.temperature = initial_temperature

            # è®­ç»ƒæ¨¡å‹
            trained_model, results = train_and_evaluate(
                model,
                pos_path=pos_path,
                bg_path=bg_path,
                test_path=test_path,
                output_dir=param_output_dir
            )

            # æ”¶é›†ç»“æœ
            run_result = {
                'å‚æ•°ç»„åˆ': f'm={initial_margin}, T={initial_temperature}',
                'AD_AUCåˆ†æ•°': results.get('ad_auc_score', 0),
                'æµ‹è¯•AD_AUCåˆ†æ•°': results.get('ad_auc_score_test', 0),
                'é¢„æµ‹ç²¾åº¦': results.get('train_accuracy', 0),
                'é¢„æµ‹å¯†åº¦': results.get('train_density', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡å‡å€¼': results.get('pos_prob_mean', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': results.get('pos_prob_std', 0),
                'æ­£æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': results.get('pos_median', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡å‡å€¼': results.get('bg_prob_mean', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': results.get('bg_prob_std', 0),
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': results.get('bg_median', 0),
                'æœ€ç»ˆmargin': model.margin if hasattr(model, 'margin') else initial_margin,
                'æœ€ç»ˆtemperature': model.temperature if hasattr(model, 'temperature') else initial_temperature,
                'è¾“å‡ºè·¯å¾„': param_output_dir
            }

            # æ·»åŠ æµ‹è¯•é›†ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'test_accuracy' in results:
                run_result['æµ‹è¯•é›†å‡†ç¡®ç‡'] = results['test_accuracy']
                run_result['æµ‹è¯•æ ·æœ¬æ•°'] = results['test_size']

            all_results.append(run_result)

            print(f"âœ… è®­ç»ƒå®Œæˆ - TEST-AD-AUC: {run_result['æµ‹è¯•AD_AUCåˆ†æ•°']:.4f}, "
                  f"ç²¾åº¦: {run_result['é¢„æµ‹ç²¾åº¦']:.4f}, å¯†åº¦: {run_result['é¢„æµ‹å¯†åº¦']:.4f}")

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            # è®°å½•å¤±è´¥ä¿¡æ¯
            failed_result = {
                'å‚æ•°ç»„åˆ': f'm={initial_margin}, T={initial_temperature}',
                'AD_AUCåˆ†æ•°': 0,
                'æµ‹è¯•AD_AUCåˆ†æ•°': 0,
                'é¢„æµ‹ç²¾åº¦': 0,
                'é¢„æµ‹å¯†åº¦': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡å‡å€¼': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': 0,
                'æ­£æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡å‡å€¼': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡æ ‡å‡†å·®': 0,
                'èƒŒæ™¯æ ·æœ¬æ¦‚ç‡ä¸­ä½æ•°': 0,
                'æœ€ç»ˆmargin': initial_margin,
                'æœ€ç»ˆtemperature': initial_temperature,
                'è¾“å‡ºè·¯å¾„': param_output_dir,
                'çŠ¶æ€': f'å¤±è´¥: {e}'
            }
            all_results.append(failed_result)

    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°Excel
    if all_results:
        results_df = pd.DataFrame(all_results)
        results_file = os.path.join(output_base_dir, "all_parameter_results.xlsx")
        results_df.to_excel(results_file, index=False)

        print(f"\nâœ… æ‰€æœ‰å®éªŒç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(results_file)}")

        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“ˆ å‚æ•°è°ƒä¼˜å®éªŒç»“æœ:")
        print("=" * 100)

        # åªæ˜¾ç¤ºæˆåŠŸçš„å®éªŒ
        successful_results = [r for r in all_results if r.get('æµ‹è¯•AD_AUCåˆ†æ•°', 0) > 0]

        if successful_results:
            print(f"{'å‚æ•°ç»„åˆ':<20} {'TEST-AD-AUC':<8} {'é¢„æµ‹ç²¾åº¦':<8} {'é¢„æµ‹å¯†åº¦':<8} {'æœ€ç»ˆmargin':<10} {'æœ€ç»ˆtemperature':<12}")
            print("-" * 100)

            for result in successful_results:
                print(f"{result['å‚æ•°ç»„åˆ']:<20} {result['æµ‹è¯•AD_AUCåˆ†æ•°']:.4f}   {result['é¢„æµ‹ç²¾åº¦']:.4f}    "
                      f"{result['é¢„æµ‹å¯†åº¦']:.4f}    {result['æœ€ç»ˆmargin']:.4f}      {result['æœ€ç»ˆtemperature']:.4f}")

    # ç©ºé—´é¢„æµ‹ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°ï¼‰
    if Config.EXPORT_TIFF and all_results:
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        successful_results = [r for r in all_results if r.get('AD_AUCåˆ†æ•°', 0) > 0]
        if successful_results:
            best_result = successful_results[0]  # åªæœ‰ä¸€ä¸ªå‚æ•°ç»„åˆ
            best_model_path = os.path.join(best_result['è¾“å‡ºè·¯å¾„'], "best_model.ckpt")

            if os.path.exists(best_model_path):
                print(f"\nğŸ—ºï¸  ä½¿ç”¨å‚æ•°ç»„åˆ {best_result['å‚æ•°ç»„åˆ']} è¿›è¡Œç©ºé—´é¢„æµ‹...")

                # åŠ è½½æœ€ä½³æ¨¡å‹
                best_model = AutoPEMModel.load_from_checkpoint(
                    best_model_path,
                    input_dim=X_pos.shape[1]
                )

                # æ„å»ºç‰¹å¾æ˜ å°„
                df_pos = DataReader.load_data(pos_path)
                feature_names = df_pos.columns.tolist()
                feature_mapping = {
                    feature: os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif")
                    for feature in feature_names
                    if os.path.exists(os.path.join(Config.BASE_DIR + "entropies", f"{feature}.tif"))
                }

                # åˆå§‹åŒ–å¤„ç†å™¨å¹¶æ‰§è¡Œé¢„æµ‹
                processor = RasterProcessor(best_model, feature_mapping)
                prob_tif_path = os.path.join(output_base_dir, "susceptibility_probability.tif")
                processor.predict_to_raster(prob_tif_path)
                print(f"âœ… ç©ºé—´æ¦‚ç‡åˆ†å¸ƒå·²ä¿å­˜è‡³: {os.path.abspath(prob_tif_path)}")

    end_time = time.time()
    total_time = end_time - start_time

    # æ ¼å¼åŒ–æ˜¾ç¤ºè¿è¡Œæ—¶é—´
    hours = int(total_time // 3600)
    minutes = int((total_time % 3600) // 60)
    seconds = total_time % 60

    if hours > 0:
        time_str = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ{seconds:.1f}ç§’"
    elif minutes > 0:
        time_str = f"{minutes}åˆ†é’Ÿ{seconds:.1f}ç§’"
    else:
        time_str = f"{seconds:.1f}ç§’"

    print(f"\nğŸ‰ å‚æ•°è°ƒä¼˜å®éªŒå®Œæˆï¼æ€»è¿è¡Œæ—¶é—´: {time_str}")
    print("=" * 80)

    return all_results

# ----------------------------
# ä¿®æ”¹åçš„è¶…å‚æ•°ç½‘æ ¼æœç´¢å‡½æ•°ï¼ˆåŒæ—¶è®°å½•AD-AUCå’Œæ­£æ ·æœ¬å¹³å‡æ¦‚ç‡ï¼‰
# ----------------------------
def hyperparameter_grid_search(pos_path, bg_path, test_path=None, base_output_dir="grid_search_results"):
    """æ‰§è¡Œè¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªç»„åˆåªè®­ç»ƒ1æ¬¡ï¼‰"""

    # å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
    lambda_values = [0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]
    alpha_values = [0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2]

    # å‡†å¤‡ç»“æœå­˜å‚¨
    results = []
    ad_auc_matrix = np.zeros((len(alpha_values), len(lambda_values)))
    mean_pos_prob_matrix = np.zeros((len(alpha_values), len(lambda_values)))

    print(f"ğŸ”¬ å¼€å§‹è¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆæ¯ä¸ªç»„åˆè¿›è¡Œ1æ¬¡å®éªŒï¼Œç¡®å®šæ€§è®¡ç®—ï¼‰...")
    total_combinations = len(lambda_values) * len(alpha_values)
    pbar = tqdm(total=total_combinations, desc="è¶…å‚æ•°ç½‘æ ¼æœç´¢")

    for i, alpha in enumerate(alpha_values):
        for j, lambda_val in enumerate(lambda_values):
            reset_seed()
            # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
            output_dir = os.path.join(base_output_dir, f"lambda_{lambda_val}_alpha_{alpha}")
            os.makedirs(output_dir, exist_ok=True)

            try:
                # åŠ è½½æ­£æ ·æœ¬æ•°æ®ï¼ˆç”¨äºåç»­æ¦‚ç‡è®¡ç®—ï¼‰
                df_pos = DataReader.load_data(pos_path)

                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = PEMModel(input_dim=df_pos.shape[1])

                # è®¾ç½®è¶…å‚æ•°
                model.l2w = lambda_val
                model.adjustment_rate = alpha

                # ä½¿ç”¨ç°æœ‰çš„train_and_evaluateå‡½æ•°è¿›è¡Œè®­ç»ƒ
                trained_model, result = train_and_evaluate(
                    model,
                    pos_path=pos_path,
                    bg_path=bg_path,
                    test_path=test_path,  # ä¼ å…¥æµ‹è¯•é›†è·¯å¾„
                    output_dir=output_dir
                )
                # è·å–AD-AUCåˆ†æ•°å’Œæ­£æ ·æœ¬å¹³å‡æ¦‚ç‡
                test_ad_auc = result.get('ad_auc_score_test', 0.5)
                mean_pos_prob = result.get('pos_prob_mean', 0)
                mean_bg_prob = result.get('bg_prob_mean', 0)
                # è®°å½•å•æ¬¡è¿è¡Œç»“æœ
                results.append({
                    'lambda': lambda_val,
                    'alpha': alpha,
                    'run_id': 1,  # ç¡®å®šæ€§è®¡ç®—ï¼Œåªè¿è¡Œ1æ¬¡
                    'ad_auc': test_ad_auc,
                    'mean_pos_prob': mean_pos_prob,
                    'mean_bg_prob': mean_bg_prob,
                    'train_accuracy': result.get('train_accuracy', 0),
                    'train_density': result.get('train_density', 0),
                    'output_dir': output_dir
                })

                # å­˜å‚¨åˆ°çŸ©é˜µä¸­
                ad_auc_matrix[i, j] = test_ad_auc
                mean_pos_prob_matrix[i, j] = mean_pos_prob

                pbar.set_description(f"Î»={lambda_val}, Î±={alpha}, TEST-AD-AUC={test_ad_auc:.4f}, MeanP={mean_pos_prob:.4f}")
                pbar.update(1)

            except Exception as e:
                print(f"é”™è¯¯: Î»={lambda_val}, Î±={alpha}, é”™è¯¯ä¿¡æ¯: {e}")
                # å¦‚æœè¿è¡Œå¤±è´¥ï¼Œç”¨NaNå¡«å……
                results.append({
                    'lambda': lambda_val,
                    'alpha': alpha,
                    'run_id': 1,
                    'ad_auc': np.nan,
                    'mean_pos_prob': np.nan,
                    'mean_bg_prob': np.nan,
                    'train_accuracy': np.nan,
                    'train_density': np.nan,
                    'output_dir': output_dir
                })
                ad_auc_matrix[i, j] = np.nan
                mean_pos_prob_matrix[i, j] = np.nan
                pbar.update(1)
                continue

    pbar.close()

    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    results_df = pd.DataFrame(results)

    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡è¡¨ï¼ˆç”±äºæ˜¯ç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªç»„åˆåªæœ‰ä¸€æ¬¡è¿è¡Œï¼‰
    summary_df = results_df.groupby(['lambda', 'alpha']).agg({
        'ad_auc': ['mean', 'min', 'max'],
        'mean_pos_prob': ['mean', 'min', 'max'],
        'mean_bg_prob': ['mean', 'min', 'max'],
        'train_accuracy': 'mean',
        'train_density': 'mean'
    }).round(4)

    # æ‰å¹³åŒ–åˆ—å
    summary_df.columns = [
        'ad_auc_mean', 'ad_auc_min', 'ad_auc_max',
        'mean_pos_prob_mean', 'mean_pos_prob_min', 'mean_pos_prob_max',
        'mean_bg_prob_mean', 'mean_bg_prob_min', 'mean_bg_prob_max',
        'train_accuracy_mean', 'train_density_mean'
    ]
    summary_df = summary_df.reset_index()

    # æ‰¾å‡ºæœ€ä½³å‚æ•°ç»„åˆ
    valid_results = summary_df[summary_df['ad_auc_mean'].notna()]
    if not valid_results.empty:
        best_idx = valid_results['ad_auc_mean'].idxmax()
        best_combo = valid_results.loc[best_idx]

        print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ:")
        print(f"â— Î» = {best_combo['lambda']}, Î± = {best_combo['alpha']}")
        print(f"â— TEST-AD-AUC: {best_combo['ad_auc_mean']:.4f}")
        print(f"â— æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {best_combo['mean_pos_prob_mean']:.4f}")

    return (summary_df, results_df, ad_auc_matrix, mean_pos_prob_matrix,
            lambda_values, alpha_values)


def plot_comprehensive_heatmaps(ad_auc_matrix, mean_pos_prob_matrix,
                                lambda_values, alpha_values,
                                output_path="comprehensive_hyperparameter_analysis.png"):
    """ç»˜åˆ¶åŒ…å«AD-AUCå’Œæ­£æ ·æœ¬å¹³å‡æ¦‚ç‡çš„ç»¼åˆçƒ­åŠ›å›¾ï¼ˆSCIæ ¼å¼ï¼Œç®€åŒ–ç‰ˆï¼‰"""

    # è®¾ç½®SCIè®ºæ–‡çš„ç»˜å›¾é£æ ¼
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['font.size'] = 11

    # åˆ›å»º2x1çš„å­å›¾å¸ƒå±€ï¼Œä¸“æ³¨äºä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # å­å›¾1ï¼šå¹³å‡AD-AUCçƒ­åŠ›å›¾
    if not np.isnan(ad_auc_matrix).all():
        ad_auc_vmin = np.nanmin(ad_auc_matrix)
        ad_auc_vmax = np.nanmax(ad_auc_matrix)
    else:
        ad_auc_vmin, ad_auc_vmax = 0.5, 1.0  # é»˜è®¤èŒƒå›´

    im1 = axes[0].imshow(ad_auc_matrix, cmap='viridis', aspect='auto',
                         vmin=ad_auc_vmin, vmax=ad_auc_vmax)

    # è®¾ç½®åˆ»åº¦æ ‡ç­¾
    for ax in axes:
        ax.set_xticks(np.arange(len(lambda_values)))
        ax.set_yticks(np.arange(len(alpha_values)))
        ax.set_xticklabels([f"{l:.3f}" for l in lambda_values])
        ax.set_yticklabels([f"{a:.3f}" for a in alpha_values])

    # åœ¨AD-AUCçƒ­åŠ›å›¾ä¸­æ˜¾ç¤ºæ•°å€¼
    for i in range(len(alpha_values)):
        for j in range(len(lambda_values)):
            if not np.isnan(ad_auc_matrix[i, j]):
                text_color = "white" if ad_auc_matrix[i, j] < (ad_auc_vmin + ad_auc_vmax) * 0.5 else "black"
                text = axes[0].text(j, i, f'{ad_auc_matrix[i, j]:.2f}',
                                    ha="center", va="center", color=text_color, fontsize=9,
                                    fontweight='bold')

    axes[0].set_xlabel('Regularization Coefficient (Î»)', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Adaptation Rate (Î±)', fontsize=12, fontweight='bold')
    axes[0].set_title('(a) Testing AD-AUC Performance\n(Higher is better)',
                      fontsize=13, fontweight='bold', pad=15)

    # é«˜äº®æœ€ä½³AD-AUC
    if not np.isnan(ad_auc_matrix).all():
        best_idx = np.unravel_index(np.nanargmax(ad_auc_matrix), ad_auc_matrix.shape)
        axes[0].add_patch(plt.Rectangle((best_idx[1] - 0.5, best_idx[0] - 0.5), 1, 1,
                                        fill=False, edgecolor='red', lw=2, linestyle='--'))
        # æ·»åŠ æœ€ä½³å€¼æ ‡æ³¨
        axes[0].text(best_idx[1], best_idx[0] + 0.3, 'Best',
                     ha='center', va='bottom', color='red', fontsize=8, fontweight='bold')

    # å­å›¾2ï¼šå¹³å‡æ­£æ ·æœ¬æ¦‚ç‡çƒ­åŠ›å›¾
    if not np.isnan(mean_pos_prob_matrix).all():
        prob_vmin = np.nanmin(mean_pos_prob_matrix)
        prob_vmax = np.nanmax(mean_pos_prob_matrix)
    else:
        prob_vmin, prob_vmax = 0.0, 1.0  # é»˜è®¤èŒƒå›´

    im2 = axes[1].imshow(mean_pos_prob_matrix, cmap='RdYlGn_r', aspect='auto',
                         vmin=prob_vmin, vmax=prob_vmax)

    # åœ¨æ­£æ ·æœ¬æ¦‚ç‡çƒ­åŠ›å›¾ä¸­æ˜¾ç¤ºæ•°å€¼
    for i in range(len(alpha_values)):
        for j in range(len(lambda_values)):
            if not np.isnan(mean_pos_prob_matrix[i, j]):
                text_color = "black"
                text = axes[1].text(j, i, f'{mean_pos_prob_matrix[i, j]:.2f}',
                                    ha="center", va="center", color=text_color, fontsize=9,
                                    fontweight='bold')

    axes[1].set_xlabel('Regularization Coefficient (Î»)', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('Adaptation Rate (Î±)', fontsize=12, fontweight='bold')
    axes[1].set_title('(b) Positive Sample Probability\n(Higher is better)',
                      fontsize=13, fontweight='bold', pad=15)

    # é«˜äº®æœ€é«˜æ­£æ ·æœ¬æ¦‚ç‡
    if not np.isnan(mean_pos_prob_matrix).all():
        best_prob_idx = np.unravel_index(np.nanargmax(mean_pos_prob_matrix), mean_pos_prob_matrix.shape)
        axes[1].add_patch(plt.Rectangle((best_prob_idx[1] - 0.5, best_prob_idx[0] - 0.5), 1, 1,
                                        fill=False, edgecolor='blue', lw=2, linestyle='--'))
        # æ·»åŠ æœ€ä½³å€¼æ ‡æ³¨
        axes[1].text(best_prob_idx[1], best_prob_idx[0] + 0.3, 'Best',
                     ha='center', va='bottom', color='blue', fontsize=8, fontweight='bold')

    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im1, ax=axes[0], shrink=0.8, label='AD-AUC Score', pad=0.05)
    plt.colorbar(im2, ax=axes[1], shrink=0.8, label='Probability', pad=0.05)

    # æ·»åŠ æ•´ä½“æ ‡é¢˜
    plt.suptitle('Hyperparameter Grid Search Results', fontsize=16, fontweight='bold', y=0.95)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()

    # è¾“å‡ºæœ€ä½³å‚æ•°ç»„åˆä¿¡æ¯
    if not np.isnan(ad_auc_matrix).all() and not np.isnan(mean_pos_prob_matrix).all():
        best_auc_idx = np.unravel_index(np.nanargmax(ad_auc_matrix), ad_auc_matrix.shape)
        best_prob_idx = np.unravel_index(np.nanargmax(mean_pos_prob_matrix), mean_pos_prob_matrix.shape)

        best_auc_lambda = lambda_values[best_auc_idx[1]]
        best_auc_alpha = alpha_values[best_auc_idx[0]]
        best_auc_value = ad_auc_matrix[best_auc_idx]

        best_prob_lambda = lambda_values[best_prob_idx[1]]
        best_prob_alpha = alpha_values[best_prob_idx[0]]
        best_prob_value = mean_pos_prob_matrix[best_prob_idx]

        print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆåˆ†æ:")
        print(f"â— æœ€é«˜TEST-AD-AUC: Î»={best_auc_lambda:.3f}, Î±={best_auc_alpha:.3f} (AD-AUC: {best_auc_value:.4f})")
        print(f"â— æœ€é«˜æ­£æ ·æœ¬æ¦‚ç‡: Î»={best_prob_lambda:.3f}, Î±={best_prob_alpha:.3f} (æ¦‚ç‡: {best_prob_value:.4f})")

    return fig

def run_comprehensive_grid_search():
    """è¿è¡Œç»¼åˆç½‘æ ¼æœç´¢ï¼ŒåŒæ—¶è€ƒè™‘AD-AUCå’Œæ­£æ ·æœ¬å¹³å‡æ¦‚ç‡"""
    start_time = time.time()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    bg_path = Config.BASE_DIR + "mesh3000.xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"
    base_output_dir = "result/pem_positive/grid_search_results"

    print(f"ğŸ”¬ å¼€å§‹ç»¼åˆè¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆå¹³è¡¡AD-AUCä¸æ­£æ ·æœ¬æ¦‚ç‡ï¼‰...")

    # æ‰§è¡Œä¿®æ”¹åçš„ç½‘æ ¼æœç´¢
    (summary_df, detailed_df, ad_auc_matrix, mean_pos_prob_matrix,
      lambda_values, alpha_values) = hyperparameter_grid_search(
        pos_path=pos_path,
        bg_path=bg_path,
        test_path=test_path,
        base_output_dir=base_output_dir,
    )

    # ç»˜åˆ¶ç»¼åˆçƒ­åŠ›å›¾ - ç°åœ¨ä¼ å…¥æ‰€æœ‰4ä¸ªçŸ©é˜µ
    heatmap_path = os.path.join(base_output_dir, "comprehensive_hyperparameter_analysis.png")
    fig = plot_comprehensive_heatmaps(
        ad_auc_matrix=ad_auc_matrix,
        mean_pos_prob_matrix=mean_pos_prob_matrix,
        lambda_values=lambda_values,
        alpha_values=alpha_values,
        output_path=heatmap_path
    )
    # å¤šå‡†åˆ™å‚æ•°é€‰æ‹©
    if not summary_df.empty:
        # å‡†åˆ™1ï¼šAD-AUCæœ€é«˜
        best_ad_auc = summary_df.loc[summary_df['ad_auc_mean'].idxmax()]

        # å‡†åˆ™2ï¼šæ­£æ ·æœ¬å¹³å‡æ¦‚ç‡æœ€é«˜ï¼ˆä¸”å¤§äº0.7ï¼‰
        high_prob_df = summary_df[summary_df['mean_pos_prob_mean'] > 0.7]
        if not high_prob_df.empty:
            best_prob = high_prob_df.loc[high_prob_df['mean_pos_prob_mean'].idxmax()]
        else:
            best_prob = summary_df.loc[summary_df['mean_pos_prob_mean'].idxmax()]

        # å‡†åˆ™3ï¼šå¹³è¡¡é€‰æ‹©ï¼ˆåœ¨æ­£æ ·æœ¬æ¦‚ç‡>0.8çš„æ¡ä»¶ä¸‹ï¼Œé€‰æ‹©AD-AUCæœ€é«˜ï¼‰
        if not high_prob_df.empty:
            balanced_choice = high_prob_df.loc[high_prob_df['ad_auc_mean'].idxmax()]
        else:
            # å¦‚æœæ²¡æœ‰æ»¡è¶³>0.7çš„ï¼Œé€‰æ‹©æœ€æ¥è¿‘0.7çš„
            summary_df['prob_diff'] = abs(summary_df['mean_pos_prob_mean'] - 0.7)
            balanced_choice = summary_df.loc[summary_df['prob_diff'].idxmin()]

        print("\n" + "=" * 80)
        print("â­ ç»¼åˆè¶…å‚æ•°ç½‘æ ¼æœç´¢ç»“æœåˆ†æ â­")
        print("=" * 80)
        print("1. åŸºäºå•ä¸€å‡†åˆ™çš„æœ€ä¼˜å‚æ•°ï¼š")
        print(f"   - æœ€é«˜AD-AUC: Î»={best_ad_auc['lambda']:.3f}, Î±={best_ad_auc['alpha']:.3f}")
        print(f"     AD-AUC: {best_ad_auc['ad_auc_mean']:.4f}")
        print(f"     æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {best_ad_auc['mean_pos_prob_mean']:.4f}")

        print(f"   - æœ€é«˜æ­£æ ·æœ¬æ¦‚ç‡: Î»={best_prob['lambda']:.3f}, Î±={best_prob['alpha']:.3f}")
        print(f"     AD-AUC: {best_prob['ad_auc_mean']:.4f}")
        print(f"     æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {best_prob['mean_pos_prob_mean']:.4f}")

        print("\n2. åŸºäºå¤šå‡†åˆ™å¹³è¡¡é€‰æ‹©ï¼ˆæ¨èï¼‰ï¼š")
        print(f"   - å¹³è¡¡é€‰æ‹©: Î»={balanced_choice['lambda']:.3f}, Î±={balanced_choice['alpha']:.3f}")
        print(f"     AD-AUC: {balanced_choice['ad_auc_mean']:.4f}")
        print(f"     æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {balanced_choice['mean_pos_prob_mean']:.4f}")

        # ä¿å­˜ç»“æœ
        results_csv_path = os.path.join(base_output_dir, "comprehensive_grid_search_results.csv")
        summary_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_csv_path}")

    end_time = time.time()
    total_time = (end_time - start_time) / 60
    print(f"\nâ±ï¸ æ€»è¿è¡Œæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
    print("âœ… ç»¼åˆç½‘æ ¼æœç´¢å®Œæˆï¼")


def lambda_grid_search(pos_path, bg_path, test_path=None, base_output_dir="lambda_grid_search_results"):
    """æ‰§è¡Œlambdaè¶…å‚æ•°ç½‘æ ¼æœç´¢ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªlambdaåªè®­ç»ƒ1æ¬¡ï¼‰"""
    lambda_values = [0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04,
                     0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]
    # å‡†å¤‡ç»“æœå­˜å‚¨
    results = []
    ad_auc_list = []
    pos_prob_list = []
    bg_prob_list = []
    energy_gap_list = []  # æ­£è´Ÿæ ·æœ¬èƒ½é‡å·®
    pos_energy_list = []  # æ­£æ ·æœ¬å¹³å‡èƒ½é‡
    bg_energy_list = []  # èƒŒæ™¯æ ·æœ¬å¹³å‡èƒ½é‡

    print(f"ğŸ”¬ å¼€å§‹Lambdaè¶…å‚æ•°æœç´¢ï¼ˆ{len(lambda_values)}ä¸ªå€¼ï¼‰...")

    total_lambdas = len(lambda_values)
    pbar = tqdm(total=total_lambdas, desc="Lambdaæœç´¢è¿›åº¦")

    for lambda_idx, lambda_val in enumerate(lambda_values):
        reset_seed()  # ç¡®ä¿å¯é‡å¤æ€§

        # ä¸ºæ¯ä¸ªlambdaåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        output_dir = os.path.join(base_output_dir, f"lambda_{lambda_val:.4f}")
        os.makedirs(output_dir, exist_ok=True)

        try:
            # åŠ è½½æ­£æ ·æœ¬æ•°æ®
            df_pos = DataReader.load_data(pos_path)

            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = PEMModel(input_dim=df_pos.shape[1])

            # è®¾ç½®è¶…å‚æ•°
            model.l2w = lambda_val

            # ä½¿ç”¨ç°æœ‰çš„train_and_evaluateå‡½æ•°è¿›è¡Œè®­ç»ƒ
            trained_model, result = train_and_evaluate(
                model,
                pos_path=pos_path,
                bg_path=bg_path,
                test_path=test_path,
                output_dir=output_dir
            )

            # è·å–å„é¡¹æŒ‡æ ‡
            test_ad_auc = result.get('ad_auc_score_test', 0.5)
            mean_pos_prob = result.get('pos_prob_mean', 0)
            mean_bg_prob = result.get('bg_prob_mean', 0)

            # è·å–èƒ½é‡å€¼ï¼ˆå¦‚æœæ¨¡å‹æä¾›ï¼‰
            mean_pos_energy = result.get('pos_energy_mean', 0)
            mean_bg_energy = result.get('bg_energy_mean', 0)
            energy_gap = mean_bg_energy - mean_pos_energy  # èƒ½é‡å·® = èƒŒæ™¯èƒ½é‡ - æ­£æ ·æœ¬èƒ½é‡

            # è®°å½•ç»“æœ
            result_dict = {
                'lambda': lambda_val,
                'ad_auc': test_ad_auc,
                'mean_pos_prob': mean_pos_prob,
                'mean_bg_prob': mean_bg_prob,
                'mean_pos_energy': mean_pos_energy,
                'mean_bg_energy': mean_bg_energy,
                'energy_gap': energy_gap,
                'train_accuracy': result.get('train_accuracy', 0),
                'train_density': result.get('train_density', 0),
                'output_dir': output_dir
            }

            results.append(result_dict)

            # å­˜å‚¨åˆ°åˆ—è¡¨ç”¨äºç»˜å›¾
            ad_auc_list.append(test_ad_auc)
            pos_prob_list.append(mean_pos_prob)
            bg_prob_list.append(mean_bg_prob)
            pos_energy_list.append(mean_pos_energy)
            bg_energy_list.append(mean_bg_energy)
            energy_gap_list.append(energy_gap)

            pbar.set_description(f"Î»={lambda_val:.4f}, AD-AUC={test_ad_auc:.4f}, "
                                 f"P(pos)={mean_pos_prob:.4f}, Î”E={energy_gap:.4f}")
            pbar.update(1)

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: Î»={lambda_val:.4f}, é”™è¯¯ä¿¡æ¯: {e}")
            # å¦‚æœè¿è¡Œå¤±è´¥ï¼Œç”¨NaNå¡«å……
            result_dict = {
                'lambda': lambda_val,
                'ad_auc': np.nan,
                'mean_pos_prob': np.nan,
                'mean_bg_prob': np.nan,
                'mean_pos_energy': np.nan,
                'mean_bg_energy': np.nan,
                'energy_gap': np.nan,
                'train_accuracy': np.nan,
                'train_density': np.nan,
                'output_dir': output_dir
            }

            results.append(result_dict)
            ad_auc_list.append(np.nan)
            pos_prob_list.append(np.nan)
            bg_prob_list.append(np.nan)
            pos_energy_list.append(np.nan)
            bg_energy_list.append(np.nan)
            energy_gap_list.append(np.nan)

            pbar.update(1)
            continue

    pbar.close()

    # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
    results_df = pd.DataFrame(results)

    # æ‰¾å‡ºæœ€ä½³lambdaï¼ˆåŸºäºAD-AUCï¼‰
    valid_results = results_df[results_df['ad_auc'].notna()]
    if not valid_results.empty:
        best_idx = valid_results['ad_auc'].idxmax()
        best_lambda = valid_results.loc[best_idx]

        print(f"\nğŸ† æœ€ä½³å‚æ•°:")
        print(f"â— Î» = {best_lambda['lambda']:.6f}")
        print(f"â— TEST-AD-AUC: {best_lambda['ad_auc']:.4f}")
        print(f"â— æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {best_lambda['mean_pos_prob']:.4f}")
        print(f"â— èƒ½é‡å·®(Î”E): {best_lambda['energy_gap']:.4f}")
        print(f"â— æ­£æ ·æœ¬å¹³å‡èƒ½é‡: {best_lambda['mean_pos_energy']:.4f}")
        print(f"â— èƒŒæ™¯æ ·æœ¬å¹³å‡èƒ½é‡: {best_lambda['mean_bg_energy']:.4f}")

    return (results_df, lambda_values, ad_auc_list, pos_prob_list,
            bg_prob_list, pos_energy_list, bg_energy_list, energy_gap_list)


def run_lambda_grid_search():
    """è¿è¡ŒLambdaå‚æ•°ç½‘æ ¼æœç´¢"""
    start_time = time.time()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE + ".xlsx"
    bg_path = Config.BASE_DIR + "mesh3000.xlsx"
    test_path = Config.BASE_DIR + "test.xlsx"
    base_output_dir = "result/pem_positive/lambda_grid_search"
    os.makedirs(base_output_dir, exist_ok=True)

    print("=" * 80)
    print("ğŸ”¬ Lambdaè¶…å‚æ•°ç½‘æ ¼æœç´¢åˆ†æ")
    print("=" * 80)

    # æ‰§è¡ŒLambdaç½‘æ ¼æœç´¢
    (results_df, lambda_values, ad_auc_list, pos_prob_list,
     bg_prob_list, pos_energy_list, bg_energy_list, energy_gap_list) = lambda_grid_search(
        pos_path=pos_path,
        bg_path=bg_path,
        test_path=test_path,
        base_output_dir=base_output_dir
    )

    # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨æ ¼
    summary_df = results_df[['lambda', 'ad_auc', 'mean_pos_prob', 'mean_bg_prob',
                             'mean_pos_energy', 'mean_bg_energy', 'energy_gap',
                             'train_accuracy', 'train_density']].copy()

    # æŒ‰AD-AUCæ’åº
    summary_df = summary_df.sort_values('ad_auc', ascending=False)

    # è¾“å‡ºè¯¦ç»†ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š Lambdaè¶…å‚æ•°æœç´¢ç»“æœæ±‡æ€»")
    print("=" * 80)
    print("\næŒ‰AD-AUCæ’åºï¼ˆå‰10åï¼‰:")
    print("-" * 120)
    print(
        f"{'Lambda':<10} {'AD-AUC':<10} {'P(pos)':<10} {'P(bg)':<10} {'E(pos)':<10} {'E(bg)':<10} {'Î”E':<10} {'A_train':<10} {'D_train':<10}")
    print("-" * 120)

    for idx, row in summary_df.head(10).iterrows():
        print(f"{row['lambda']:<10.6f} {row['ad_auc']:<10.4f} {row['mean_pos_prob']:<10.4f} "
              f"{row['mean_bg_prob']:<10.4f} {row['mean_pos_energy']:<10.4f} "
              f"{row['mean_bg_energy']:<10.4f} {row['energy_gap']:<10.4f} "
              f"{row['train_accuracy']:<10.4f} {row['train_density']:<10.4f}")

    # è¾“å‡ºå®Œæ•´ç»“æœåˆ°CSV
    csv_path = os.path.join(base_output_dir, "lambda_grid_search_results.csv")
    results_df.to_csv(csv_path, index=False, encoding='utf-8-sig')

    # è¾“å‡ºæœ€ä½³å‚æ•°å»ºè®®
    best_row = summary_df.iloc[0]
    print("\n" + "=" * 80)
    print("ğŸ’¡ æœ€ä½³å‚æ•°æ¨èï¼ˆåŸºäºAD-AUCï¼‰")
    print("=" * 80)
    print(f"æ¨è Î» = {best_row['lambda']:.6f}")
    print(f"ç†ç”±:")
    print(f"1. AD-AUCæœ€é«˜: {best_row['ad_auc']:.4f}")
    print(f"2. æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {best_row['mean_pos_prob']:.4f}")
    print(f"3. èƒ½é‡å·®(Î”E): {best_row['energy_gap']:.4f} (è¶Šé«˜è¡¨ç¤ºæ¨¡å‹åˆ¤åˆ«åŠ›è¶Šå¼º)")

    # åˆ†æÎ»çš„å½±å“è¶‹åŠ¿
    print("\nğŸ“ˆ Lambdaå‚æ•°å½±å“è¶‹åŠ¿åˆ†æ:")
    print("-" * 60)

    # å°†æ•°æ®æŒ‰Î»æ’åº
    trend_df = results_df.sort_values('lambda').dropna()
    if len(trend_df) > 1:
        # è®¡ç®—ç›¸å…³ç³»æ•°
        corr_auc = trend_df['lambda'].corr(trend_df['ad_auc'])
        corr_prob = trend_df['lambda'].corr(trend_df['mean_pos_prob'])
        corr_gap = trend_df['lambda'].corr(trend_df['energy_gap'])

        print(f"Lambdaä¸AD-AUCçš„ç›¸å…³ç³»æ•°: {corr_auc:.4f}")
        print(f"Lambdaä¸æ­£æ ·æœ¬æ¦‚ç‡çš„ç›¸å…³ç³»æ•°: {corr_prob:.4f}")
        print(f"Lambdaä¸èƒ½é‡å·®çš„ç›¸å…³ç³»æ•°: {corr_gap:.4f}")

        if corr_auc > 0.3:
            print("â†’ Lambdaå¢å¤§å€¾å‘äºæé«˜AD-AUC")
        elif corr_auc < -0.3:
            print("â†’ Lambdaå¢å¤§å€¾å‘äºé™ä½AD-AUC")
        else:
            print("â†’ Lambdaä¸AD-AUCæ— æ˜æ˜¾çº¿æ€§å…³ç³»")

    end_time = time.time()
    total_time = (end_time - start_time) / 60
    print(f"\nâ±ï¸ æ€»è¿è¡Œæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {base_output_dir}")
    print("âœ… Lambdaç½‘æ ¼æœç´¢å®Œæˆï¼")

    return results_df, summary_df

def run_architecture_ablation_study():
    """
    æ‰§è¡Œç½‘ç»œæ¶æ„æ¶ˆèç ”ç©¶ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªæ¶æ„åªè®­ç»ƒ1æ¬¡ï¼‰
    """
    start_time = time.time()

    # æ–‡ä»¶è·¯å¾„é…ç½®
    pos_path = Config.BASE_DIR + Config.POSITIVE+".xlsx"
    bg_path = Config.BASE_DIR + "mesh3000.xlsx"  # å›ºå®šä½¿ç”¨mesh3000ä½œä¸ºèƒŒæ™¯æ ·æœ¬
    test_path = Config.BASE_DIR + "test.xlsx"

    # å®šä¹‰è¦æ¯”è¾ƒçš„æ¨¡å‹æ¶æ„åˆ—è¡¨
    model_classes = {
        'PEM_Shallow': PEM_Shallow,
        'PEM-Base': PEMModel,  # åŸºå‡†æ¨¡å‹
        'PEM_Deep': PEM_Deep,
        'PEM_Wide': PEM_Wide,
        'PEM_Narrow': PEM_Narrow
    }

    # å®éªŒå‚æ•°ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªæ¶æ„åªè®­ç»ƒ1æ¬¡ï¼‰
    max_epochs = 1000

    # åŠ è½½æ•°æ®è·å–è¾“å…¥ç»´åº¦ï¼ˆæ‰€æœ‰æ¨¡å‹å…±äº«ï¼‰
    df_pos = DataReader.load_data(pos_path)
    input_dim = df_pos.shape[1]

    print("ğŸ”¬ å¼€å§‹ç½‘ç»œæ¶æ„æ¶ˆèç ”ç©¶ï¼ˆç¡®å®šæ€§è®¡ç®—ï¼Œæ¯ä¸ªæ¶æ„è®­ç»ƒ1æ¬¡ï¼‰...")
    print("=" * 80)
    print(f"â— å‚ä¸æ¯”è¾ƒçš„æ¶æ„æ•°é‡: {len(model_classes)}")
    print(f"â— è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
    print(f"â— èƒŒæ™¯æ ·æœ¬æ–‡ä»¶: {bg_path}")
    print("=" * 80)

    # å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœ
    all_results = []

    # å¯¹æ¯ä¸ªæ¨¡å‹æ¶æ„è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼ˆç¡®å®šæ€§è®¡ç®—ä¿è¯ç»“æœå¯é‡ç°ï¼‰
    for model_name, model_class in model_classes.items():
        reset_seed()
        print(f"\nğŸ—ï¸ æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name}")

        # ä¸ºå½“å‰æ¨¡å‹åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"result/pem_positive/architecture_ablation/{model_name}"
        os.makedirs(output_dir, exist_ok=True)

        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = model_class(input_dim=input_dim)

            # ä½¿ç”¨ç®€åŒ–è®­ç»ƒå‡½æ•°
            best_model, metrics = train_and_evaluate(
                model=model,
                pos_path=pos_path,
                bg_path=bg_path,
                test_path=test_path,
                output_dir=output_dir,
                max_epochs=max_epochs
            )

            # è®°å½•å…³é”®æŒ‡æ ‡
            model_result = {
                'model_name': model_name,
                'ad_auc_score': metrics.get('ad_auc_score', 0),
                'ad_auc_score_test': metrics.get('ad_auc_score_test', 0),
                'train_accuracy': metrics.get('train_accuracy', 0),
                'train_density': metrics.get('train_density', 0),
                'mean_pos_prob': metrics.get('pos_prob_mean', 0),
                'bg_prob_mean': metrics.get('bg_prob_mean', 0),
                'final_margin': best_model.margin if hasattr(best_model, 'margin') else 0,
                'final_temperature': best_model.temperature if hasattr(best_model, 'temperature') else 0,
                'output_dir': output_dir,
                'training_epochs': metrics.get('training_epochs', 0),
                'training_time': metrics.get('training_time', 0)
            }

            # æ·»åŠ æµ‹è¯•é›†ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'test_accuracy' in metrics:
                model_result['test_accuracy'] = metrics['test_accuracy']
                model_result['test_size'] = metrics['test_size']

            all_results.append(model_result)

            print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ")
            print(f"   â— AD-AUC: {model_result['ad_auc_score']:.4f}")
            print(f"   â— æµ‹è¯•é›†AD-AUC: {model_result['ad_auc_score_test']:.4f}")
            print(f"   â— è®­ç»ƒç²¾åº¦: {model_result['train_accuracy']:.4f}")
            print(f"   â— æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {model_result['mean_pos_prob']:.4f}")

        except Exception as e:
            print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
            # è®°å½•å¤±è´¥ä¿¡æ¯
            failed_result = {
                'model_name': model_name,
                'ad_auc_score': 0,
                'ad_auc_score_test': 0,
                'train_accuracy': 0,
                'train_density': 0,
                'mean_pos_prob': 0,
                'bg_prob_mean': 0,
                'final_margin': 0,
                'final_temperature': 0,
                'output_dir': output_dir,
                'training_epochs': 0,
                'training_time': 0,
                'error': str(e)
            }
            all_results.append(failed_result)

    # åˆ›å»ºç»“æœDataFrameå¹¶æŒ‰æ€§èƒ½æ’åº
    results_df = pd.DataFrame(all_results)

    # åªå¯¹æˆåŠŸçš„è®­ç»ƒç»“æœè¿›è¡Œæ’åº
    successful_results = results_df[results_df['ad_auc_score'] > 0]
    if not successful_results.empty:
        results_df_sorted = successful_results.sort_values('ad_auc_score', ascending=False)

        # é‡æ–°æ•´åˆå¤±è´¥çš„ç»“æœ
        failed_results = results_df[results_df['ad_auc_score'] == 0]
        results_df = pd.concat([results_df_sorted, failed_results])
    else:
        results_df = results_df.sort_values('ad_auc_score', ascending=False)

    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°Excel
    output_file = "result/pem_positive/architecture_ablation_results.xlsx"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    results_df.to_excel(output_file, index=False)

    # è¾“å‡ºå®éªŒæ€»ç»“
    end_time = time.time()
    total_time = end_time - start_time

    print(f"\nğŸ‰ ç½‘ç»œæ¶æ„æ¶ˆèç ”ç©¶å®Œæˆ!")
    print("=" * 80)

    # æ˜¾ç¤ºæˆåŠŸçš„æ¨¡å‹æ’å
    successful_models = [r for r in all_results if r.get('ad_auc_score', 0) > 0]
    if successful_models:
        print("ğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
        print("-" * 80)
        successful_models_sorted = sorted(successful_models, key=lambda x: x['ad_auc_score'], reverse=True)

        for i, result in enumerate(successful_models_sorted, 1):
            print(f"{i}. {result['model_name']}:")
            print(f"   AD-AUC: {result['ad_auc_score']:.4f}")
            print(f"   æµ‹è¯•é›†AD-AUC: {result['ad_auc_score_test']:.4f}")
            print(f"   è®­ç»ƒç²¾åº¦: {result['train_accuracy']:.4f}")
            print(f"   æ­£æ ·æœ¬å¹³å‡æ¦‚ç‡: {result['mean_pos_prob']:.4f}")
            print()

    # æ˜¾ç¤ºå¤±è´¥æ¨¡å‹
    failed_models = [r for r in all_results if r.get('ad_auc_score', 0) == 0]
    if failed_models:
        print("âŒ è®­ç»ƒå¤±è´¥çš„æ¨¡å‹:")
        print("-" * 80)
        for result in failed_models:
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"â— {result['model_name']}: {error_msg}")

    print(f"\nğŸ“ˆ å®éªŒæ¦‚å†µ:")
    print(f"â— æ€»æ¨¡å‹æ•°é‡: {len(model_classes)}")
    print(f"â— æˆåŠŸè®­ç»ƒ: {len(successful_models)}")
    print(f"â— è®­ç»ƒå¤±è´¥: {len(failed_models)}")
    print(f"â— æ€»è€—æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")
    print(f"â— ç»“æœæ–‡ä»¶: {output_file}")
    print("=" * 80)

    return results_df

# ----------------------------
# ä¸»ç¨‹åº
# ----------------------------
if __name__ == "__main__":
    seed_everything()
    run_one()
    #batch_training_experiment()
    #run_m_T()
    #run_Auto_m_T()
    #run_lambda_grid_search()
    #run_comprehensive_grid_search()
    #run_architecture_ablation_study()


